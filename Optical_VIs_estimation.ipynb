{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23815c2a-d0ed-4041-ad15-20e753122377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf24b4-9c2f-4d81-a296-14acba78d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mtp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "import os\n",
    "import glob\n",
    "from rasterio.plot import show\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08077887-0eb4-4371-a452-ad9405417839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a41cce4a-6c11-47a8-919c-2c6d4c3bab5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### The ff lines of code imports crop field boundaries and an already preprocessed time series of optical VIs and SAR data pairs.\n",
    "###### For each crop field boundary, the satellite data response are averaged to minimize SAR speckle noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718e45b-f154-4b2a-b653-ade1572d623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory\n",
    "os.chdir('C:/raster_vector')\n",
    "\n",
    "# Read file names into lists\n",
    "img_files = glob.glob('*tif')\n",
    "shapefiles = glob.glob('*shp')\n",
    "\n",
    "# Function to read data: opens .tif files and reads .shp files\n",
    "def read_data(file):\n",
    "    if file.endswith('tif'):\n",
    "        return rasterio.open(file)\n",
    "    else:\n",
    "        return gpd.read_file(file)\n",
    "\n",
    "# Store the arrays of optical/SAR images and field boundaries in lists\n",
    "opt_SAR = [read_data(img) for img in img_files]\n",
    "boundary = [read_data(vector) for vector in shapefiles]\n",
    "\n",
    "# Read rasterio objects into numpy arrays\n",
    "img_stack = [img.read() for img in opt_SAR]\n",
    "\n",
    "# Transform pixel coordinates to x, y map coordinates for each image stack\n",
    "affines = [img.transform for img in opt_SAR]\n",
    "\n",
    "# Calculate the mean RS feature value for each field boundary, for all acquisition dates\n",
    "mean = []\n",
    "for stack, poly, affine in zip(img_stack, boundary, affines):\n",
    "    for band in stack:\n",
    "        mean_df = pd.DataFrame(zonal_stats(poly, band, affine=affine, stats='mean'))\n",
    "        mean.append(mean_df)\n",
    "\n",
    "# Rename all the columns in the 'mean' list with their respective spectral band names and their acquisition date\n",
    "names = ['VV', 'VH', 'RVI', 'Ratio', 'NDVI', 'EVI']\n",
    "col_names = [filename[:-4] + '_' + name for filename in shapefiles for name in names]\n",
    "\n",
    "# Apply the new column names to the DataFrames\n",
    "for df, col_name in zip(mean, col_names):\n",
    "    df.rename(columns={'mean': col_name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471e2d3-4268-47f2-b6ef-b2b7df29bfde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3d998-1b83-4edd-8d6d-547b0239a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'mean' list into chunks of 6 items and concatenate them into dataframes\n",
    "chunks = [mean[i:i+6] for i in range(0, len(mean), 6)]\n",
    "dfs_mean = [pd.concat(chunk, axis=1) for chunk in chunks]\n",
    "\n",
    "# Filter necessary columns in the field boundary shapefiles\n",
    "field_bounds = [poly.loc[:, ['OBJECTID', 'CDL' + shape[2:6], 'geometry']] for poly, shape in zip(boundary, shapefiles)]\n",
    "\n",
    "# Append map coordinates and crop ID to each dataframe\n",
    "gdfs = [pd.concat([field, df], axis=1) for df, field in zip(dfs_mean, field_bounds)]\n",
    "\n",
    "# Add date and region columns for each dataframe using optical data acquisition date\n",
    "for df in gdfs:\n",
    "    date_str = df.columns[3][2:12]\n",
    "    df['Date'] = pd.to_datetime(date_str[:4] + '-' + date_str[5:7] + '-' + date_str[8:])\n",
    "    df['Region'] = df.columns[3][0]\n",
    "\n",
    "# Rename columns for row-wise concatenation\n",
    "new_names = ['OBJECTID', 'CDL', 'geometry', 'VV', 'VH', 'RVI', 'Ratio', 'NDVI', 'EVI', 'Date', 'Region']\n",
    "for df in gdfs:\n",
    "    df.columns = new_names\n",
    "\n",
    "# Reproject all geodataframes to EPSG 4326 and concatenate them row-wise\n",
    "gdfs_proj = [df.to_crs('epsg:4326') for df in gdfs]\n",
    "gdf = pd.concat(gdfs_proj).reset_index(drop=True)\n",
    "\n",
    "# Drop rows with NaN or VV==0.0\n",
    "gdf = gdf.dropna(subset=['VV']).loc[gdf['VV'] != 0.0].reset_index(drop=True)\n",
    "\n",
    "#NB: the values of the two columns below were split between string and integer data types, so I had to convert them into one before saving the data\n",
    "# Convert OBJECTID and CDL columns to string datatype\n",
    "gdf[['OBJECTID', 'CDL']] = gdf[['OBJECTID', 'CDL']].astype('str')\n",
    "\n",
    "# Save the final dataframe to a geoparquet file\n",
    "gdf.to_parquet('E:/New_thesis/HLS_S1_RUN/HLS_S1.geoparquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19b493-5dd3-4a86-92ba-f57a6d52cf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3550b-983a-4715-a90d-c5befcea5f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aa80d23-b24c-4bda-831d-593a4773225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load geoparquet files \n",
    "g_data = gpd.read_parquet('C:/Users/henry/Documents/New_thesis/output/HLS_S1.geoparquet')\n",
    "\n",
    "no_DC = gpd.read_parquet('E:/New_thesis/HLS_S1_RUN/no_double_cropping.geoparquet') #Occitanie data with single and double-cropping\n",
    "no_DC.rename(columns={'ID_PARCEL': 'OBJECTID'}, inplace=True)\n",
    "\n",
    "# Update CDL and Region columns\n",
    "g_data['CDL'] = g_data['CDL'].replace({'5': 'Soyabean', '1': 'Corn'})\n",
    "g_data['Region'] = g_data['Region'].replace({'P': 'Piatt', 'O': 'Occitanie', 'C': 'Crawford', 'M': 'Campo Verde'})\n",
    "\n",
    "# Filter required columns\n",
    "data = g_data[['OBJECTID', 'CDL', 'VV', 'VH', 'RVI', 'Ratio', 'NDVI', 'EVI', 'Date', 'Region']]\n",
    "\n",
    "# Add date-related columns\n",
    "data['month'] = data.Date.dt.month\n",
    "data['DOY'] = (data.Date.dt.month * 30) + data.Date.dt.day\n",
    "data['Year'] = data.Date.dt.year\n",
    "\n",
    "# Convert dB to linear scale and generate SAR ratios\n",
    "def db_to_linear(col):\n",
    "    return 10 ** (0.1 * col)\n",
    "\n",
    "data[['VH_lin', 'VV_lin']] = data[['VH', 'VV']].apply(db_to_linear)\n",
    "data['Span'] = data['VH_lin'] + data['VV_lin']\n",
    "data['Ratio_lin'] = data['VH_lin'] / data['VV_lin']\n",
    "data['RVI_lin'] = (4 * data['VH_lin']) / (data['VH_lin'] + data['VV_lin'])\n",
    "data['NRPB_lin'] = (data['VH_lin'] - data['VV_lin']) / (data['VH_lin'] + data['VV_lin'])\n",
    "\n",
    "# Drop and rename columns\n",
    "data.drop(columns=['VV', 'VH', 'RVI', 'Ratio'], inplace=True)\n",
    "data.rename(columns={\n",
    "    'CDL': 'Crop', \n",
    "    'VV_lin': 'VV', \n",
    "    'VH_lin': 'VH', \n",
    "    'RVI_lin': 'RVI', \n",
    "    'Ratio_lin': 'Ratio', \n",
    "    'span_lin': 'Span', \n",
    "    'NRPB_lin': 'NRPB'\n",
    "}, inplace=True)\n",
    "\n",
    "# group data according to region\n",
    "France_data = data[data['Region'] == 'Occitanie'].reset_index(drop=True)\n",
    "France_final = France_data.merge(no_DC, on='OBJECTID').drop(columns=['ID_PARCEL'])  # Remove the double-cropping plots data in Occitanie\n",
    "\n",
    "USA_data = data[data['Region'].isin(['Crawford', 'Piatt'])].reset_index(drop=True)\n",
    "Brazil_soya = data[(data['Region'] == 'Campo Verde') & (data['Crop'] == 'Soyabean')].reset_index(drop=True)\n",
    "Brazil_corn = data[(data['Region'] == 'Campo Verde') & (data['Crop'] == 'Corn')].reset_index(drop=True)\n",
    "\n",
    "# Load and process temperature data\n",
    "def process_temp_data(filepath):\n",
    "    temp_data = pd.read_csv(filepath, names=['Date', 'T_max', 'T_min'], skiprows=1)\n",
    "    temp_data['Date'] = pd.to_datetime(temp_data['Date'])\n",
    "    return temp_data\n",
    "\n",
    "USA_temp = process_temp_data('C:/Users/henry/OneDrive/Documents/New_thesis/USA_temp.csv')\n",
    "France_temp = process_temp_data('C:/Users/henry/OneDrive/Documents/New_thesis/France_temp.csv')\n",
    "B_soya_temp = process_temp_data('C:/Users/henry/OneDrive/Documents/New_thesis/campo_verde_2015_temp.csv')\n",
    "B_corn_temp = process_temp_data('C:/Users/henry/OneDrive/Documents/New_thesis/campo_verde_2016_temp.csv')\n",
    "\n",
    "# Compute Growing Degree Days (GDD)\n",
    "def compute_GDD(temp_data, t_base=50):\n",
    "    avg_temp = (temp_data['T_max'] + temp_data['T_min']) / 2\n",
    "    temp_data['avg'] = np.where(avg_temp > t_base, avg_temp - t_base, 0)\n",
    "    temp_data['cum_GDD'] = temp_data['avg'].cumsum()\n",
    "    return temp_data\n",
    "\n",
    "USA_temp = USA_temp.groupby(USA_temp['Date'].dt.year).apply(compute_GDD).reset_index(drop=True)\n",
    "France_temp = compute_GDD(France_temp)\n",
    "B_soya_temp = compute_GDD(B_soya_temp)\n",
    "B_corn_temp = compute_GDD(B_corn_temp)\n",
    "\n",
    "# Merge temperature data with the corresponding regions\n",
    "USA_data = USA_data.merge(USA_temp, on='Date', how='left')\n",
    "France_final = France_final.merge(France_temp, on='Date', how='left')\n",
    "Brazil_soya = Brazil_soya.merge(B_soya_temp, on='Date', how='left')\n",
    "Brazil_corn = Brazil_corn.merge(B_corn_temp, on='Date', how='left')\n",
    "\n",
    "# Combine data from all regions\n",
    "reg_data = pd.concat([USA_data, France_final, Brazil_soya, Brazil_corn], ignore_index=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "reg_data.drop(columns=['T_max', 'T_min', 'avg'], inplace=True)\n",
    "\n",
    "# Save the final dataframe to a geoparquet file\n",
    "reg_data.to_parquet('E:/New_thesis/HLS_S1_RUN/HLS_S1.geoparquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f012c8-9b65-44d5-942e-5d8a882709ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384f1cd-a239-48a3-a5b9-6356093a1725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4a3c8-70ac-4b71-83a8-05464632a0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60be79f1-b713-4ff9-a9ef-4b467f528355",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### France and Piatt 2019 Crop Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f2cc8f2-631e-48d7-b6e4-8c7785ff8d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter data for 2019\n",
    "France_Piatt_2019 = reg_data[reg_data[\"Date\"].dt.year == 2019].reset_index(drop=True)\n",
    "\n",
    "# Remove OBJECTIDs with NDVI > 0.5 on the 1st week of June or without NDVI >= 0.5 in August\n",
    "below_point_5 = France_Piatt_2019[\n",
    "    (France_Piatt_2019.NDVI <= 0.5) & \n",
    "    (France_Piatt_2019.Date.dt.month == 6) & \n",
    "    (France_Piatt_2019.Date.dt.day <= 2)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "below_point_5_all = France_Piatt_2019[\n",
    "    France_Piatt_2019['OBJECTID'].isin(below_point_5['OBJECTID'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Filter out OBJECTIDs without NDVI >= 0.5 on any acquisition date in August\n",
    "august_df = below_point_5_all[below_point_5_all['Date'].dt.month == 8]\n",
    "filtered_df = august_df.groupby('OBJECTID').filter(lambda x: x['NDVI'].max() < 0.5)\n",
    "potential_crops_2019 = below_point_5_all[\n",
    "    ~below_point_5_all['OBJECTID'].isin(filtered_df['OBJECTID'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Filter for NDVI >= 0.7 on specified dates in June (up to the 2nd)\n",
    "above_point_7 = France_Piatt_2019[\n",
    "    (France_Piatt_2019.NDVI >= 0.7) & \n",
    "    (France_Piatt_2019.Date.dt.month == 6) & \n",
    "    (France_Piatt_2019.Date.dt.day <= 2)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Remove duplicates farms from concatenated dataframe\n",
    "non_2_crops = pd.concat([above_point_7, filtered_df], axis=0).drop_duplicates(subset='OBJECTID').reset_index(drop=True)\n",
    "\n",
    "# Select all other multitemporal data for each farm in the non_2_crops dataframe\n",
    "all_non_2 = France_Piatt_2019[\n",
    "    France_Piatt_2019['OBJECTID'].isin(non_2_crops['OBJECTID'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Group by Date and Region, calculating the mean of numeric columns\n",
    "non_2_groups = all_non_2.groupby(['Date', 'Region'], as_index=False).mean(numeric_only=True).set_index('Date')\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "sns.lineplot(data=non_2_groups, x='DOY', y='NDVI', hue='Region', marker='o', ax=ax)\n",
    "ax.set_title('NDVI Trends of Potential non-Corn/Soyabean farms by Region in 2019')\n",
    "ax.set_xlabel('DOY')\n",
    "ax.set_ylabel('NDVI')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8a361-bfde-407f-a822-97fd5ee7364e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d20f6dce-1e4c-462d-930f-7d75f28f9dd8",
   "metadata": {},
   "source": [
    "##### 2023 and 2022 Crop Cleaning in Piatt County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "012eddf6-92e6-424c-ad50-a45519178d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter data for 2022 and 2023\n",
    "Piatt_2022_2023 = reg_data[reg_data[\"Date\"].dt.year >= 2022].reset_index(drop=True)\n",
    "\n",
    "# Remove OBJECTIDs with NDVI > 0.5 on the 1st week of June\n",
    "below_point_5 = Piatt_2022_2023[\n",
    "    (Piatt_2022_2023.NDVI <= 0.5) & \n",
    "    (Piatt_2022_2023.Date.dt.month == 6) & \n",
    "    (Piatt_2022_2023.Date.dt.day <= 9)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "below_point_5_all = Piatt_2022_2023[\n",
    "    Piatt_2022_2023['OBJECTID'].isin(below_point_5['OBJECTID'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Filter out OBJECTIDs without NDVI >= 0.5 on any acquisition date in August and 1st week of September 2023\n",
    "aug_sep_2023_df = below_point_5_all[\n",
    "    (below_point_5_all['Date'].dt.month.isin([8, 9])) & \n",
    "    (below_point_5_all['Date'].dt.year == 2023)\n",
    "]\n",
    "\n",
    "filtered_df = aug_sep_2023_df.groupby('OBJECTID').filter(lambda x: x['NDVI'].max() < 0.5)\n",
    "\n",
    "potential_crops_2022_2023 = below_point_5_all[\n",
    "    ~below_point_5_all['OBJECTID'].isin(filtered_df['OBJECTID'])\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49208003-9919-4f33-b4fa-1810efe4a9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec96b0c1-e2f1-4e75-9d69-f8becb33613a",
   "metadata": {},
   "source": [
    "##### Crop Cleaning for 2020 and 2021 Piatt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d5b2ef9-4cf7-495f-98fe-5f28a41128d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter data for 2020 and 2021\n",
    "Piatt_2020_2021 = reg_data[reg_data[\"Date\"].dt.year.isin([2020, 2021])].reset_index(drop=True)\n",
    "\n",
    "# Filter data to only include rows with a Date in August\n",
    "august_df = Piatt_2020_2021[Piatt_2020_2021['Date'].dt.month == 8]\n",
    "\n",
    "# Filter out OBJECTIDs without NDVI >= 0.5 on any acquisition date in August\n",
    "filtered_df = august_df.groupby('OBJECTID').filter(lambda x: x['NDVI'].max() < 0.5)\n",
    "\n",
    "# Keep only rows that are not in filtered_df\n",
    "potential_crops_2020_2021 = Piatt_2020_2021[\n",
    "    ~Piatt_2020_2021['OBJECTID'].isin(filtered_df['OBJECTID'])\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f069c6-dd79-467a-97b6-eae3b85ff536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "363e820c-2c29-43da-a86e-3a7e7b6937e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate all crop-cleaned data from Piatt and France, and previous data from Crawford County and Brazil\n",
    "reg_data_final = pd.concat([\n",
    "    potential_crops_2019,\n",
    "    potential_crops_2020_2021,\n",
    "    potential_crops_2022_2023,\n",
    "    reg_data[reg_data['Region'].isin(['Crawford', 'Campo Verde'])]\n",
    "]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf886b-148a-4e86-b41a-cde60069f9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f1cfa-35cd-4427-8840-7df46f781254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38150214-d568-43b1-aba3-b01dc64f8cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e711650-7545-4446-a7a9-d3c6ff397015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the year 2019 and concatenate with specific region data\n",
    "data_2019 = reg_data_final[reg_data_final[\"Date\"].dt.year == 2019]\n",
    "data_2019_b_soya = pd.concat([data_2019, reg_data_final[reg_data_final.Region == 'Campo Verde']], ignore_index=True)\n",
    "\n",
    "# Group by Date, Crop, and Region and calculate the mean\n",
    "dt_data = data_2019_b_soya.groupby(['Date', 'Crop', 'Region'], as_index=False).mean(numeric_only=True).set_index('Date')\n",
    "\n",
    "# Group data by Crop type\n",
    "CDL_group = dt_data.groupby('Crop')\n",
    "\n",
    "# Select Corn and Soybean data\n",
    "koo_corn = CDL_group.get_group('Corn')\n",
    "koo_soybean = CDL_group.get_group('Soyabean')\n",
    "\n",
    "# Features to plot\n",
    "x_axis = ['DOY', 'cum_GDD']\n",
    "\n",
    "# Create a figure with 4 subplots: Corn on the first row, Soybean on the second row\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Loop through Corn and Soybean data to create subplots\n",
    "for idx, variable in enumerate(x_axis):\n",
    "    sns.lineplot(data=koo_corn, x=variable, y='NDVI', hue='Region', marker='o', markersize=5, ax=axs[0, idx], legend=False)\n",
    "    sns.lineplot(data=koo_soybean, x=variable, y='NDVI', hue='Region', marker='o', markersize=5, ax=axs[1, idx], legend=True)\n",
    "\n",
    "# Set titles and grid for subplots\n",
    "for i, crop in enumerate(['Corn', 'Soyabean']):\n",
    "    for j in range(2):\n",
    "        axs[i, j].set_title(crop)\n",
    "        axs[i, j].set_xlabel('')\n",
    "        axs[i, j].set_ylabel('')\n",
    "        axs[i, j].grid(True)\n",
    "\n",
    "# Add a single y-axis label for the entire figure\n",
    "fig.text(0.005, 0.5, 'NDVI', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "# Add x-axis labels\n",
    "fig.text(0.25, 0.01, 'DOY', ha='center', va='center', fontsize=12)\n",
    "fig.text(0.75, 0.01, 'cum_GDD', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23d7cd-100b-4cc0-b181-49716d8a5b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ecde6-5dd7-442c-91b6-65b9fc318504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'Date', 'Crop', and 'Region'\n",
    "date_group = reg_data_final.groupby(['Date', 'Crop', 'Region'], as_index=False).mean(numeric_only=True).set_index('Date')\n",
    "date_group['Year'] = date_group['Year'].astype(int)\n",
    "reg_crop_group = date_group.groupby(['Crop', 'Region'])\n",
    "\n",
    "# Get Corn and Soybean groups for 'Piatt' region\n",
    "corn_df = reg_crop_group.get_group(('Corn', 'Piatt'))\n",
    "soybean_df = reg_crop_group.get_group(('Soyabean', 'Piatt'))\n",
    "\n",
    "# Group by 'Year' for both Corn and Soybean\n",
    "corn_masa = corn_df.groupby('Year')\n",
    "soybean_masa = soybean_df.groupby('Year')\n",
    "\n",
    "# Plotting variables and confidence intervals\n",
    "plotting = ['NDVI', 'EVI', 'VH', 'RVI']\n",
    "conf_interval = [0.05, 0.05, 0.003, 0.025]\n",
    "\n",
    "# Create a figure with subplots (4 rows, 2 columns)\n",
    "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(16, 10))\n",
    "\n",
    "# Loop through plotting variables for Corn and Soybean\n",
    "for idx, variable in enumerate(plotting):\n",
    "    # Corn subplots (left side)\n",
    "    for name, group in corn_masa:\n",
    "        upper = group[variable] + conf_interval[idx]\n",
    "        lower = group[variable] - conf_interval[idx]\n",
    "        sns.lineplot(data=group, x='DOY', y=variable, label=name, marker='o', markersize=5, ax=axs[idx, 0])\n",
    "        axs[idx, 0].fill_between(group['DOY'], lower, upper, alpha=0.1)\n",
    "    axs[idx, 0].set_xlim(180, 310)\n",
    "    axs[idx, 0].set_title(f'Corn - {variable}')\n",
    "    axs[idx, 0].grid(True)\n",
    "\n",
    "    # Soybean subplots (right side)\n",
    "    for name, group in soybean_masa:\n",
    "        upper = group[variable] + conf_interval[idx]\n",
    "        lower = group[variable] - conf_interval[idx]\n",
    "        sns.lineplot(data=group, x='DOY', y=variable, label=name, marker='o', markersize=5, ax=axs[idx, 1])\n",
    "        axs[idx, 1].fill_between(group['DOY'], lower, upper, alpha=0.1)\n",
    "    axs[idx, 1].set_xlim(180, 310)\n",
    "    axs[idx, 1].set_title(f'Soyabean - {variable}')\n",
    "    axs[idx, 1].grid(True)\n",
    "\n",
    "# Collect handles and labels for the legend from the last subplot\n",
    "handles, labels = axs[-1, 0].get_legend_handles_labels()\n",
    "handles_soy, labels_soy = axs[-1, 1].get_legend_handles_labels()\n",
    "handles.extend(handles_soy)\n",
    "labels.extend(labels_soy)\n",
    "\n",
    "# Remove legends from individual subplots\n",
    "for ax in axs.flatten():\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "# Add a single legend for the entire figure\n",
    "fig.legend(handles=handles, labels=labels, loc='lower center', ncol=len(corn_masa), bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "# Add 'DOY' text at the bottom center of the figure\n",
    "fig.text(0.5, 0.01, 'DOY', ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a1888-c044-44a4-ad01-b143c2d3d4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fab9d9-b2cc-48b1-beeb-070b5280ef0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4e812d23-e390-48a8-83c4-198afd93dfaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove outliers based on the Interquartile Range (IQR)\n",
    "def ignore_outliers(df, ft):\n",
    "    q1, q3 = np.percentile(df[ft], [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return df[(df[ft] > lower_bound) & (df[ft] < upper_bound)]\n",
    "\n",
    "# Group data by 'Crop'\n",
    "CDL_group = reg_data_final.groupby('Crop')\n",
    "\n",
    "# Process each group to remove outliers for specific columns\n",
    "cdls = []\n",
    "for name, group in CDL_group:\n",
    "    for column in ['VV', 'VH']:\n",
    "        masa = ignore_outliers(group, column)\n",
    "        cdls.append(masa)\n",
    "\n",
    "# Combine the cleaned data\n",
    "clean_data = pd.concat(cdls).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c099ed-3ab3-4b24-bfb0-aa7de5876885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b4bde-1874-4b70-ae97-f50e4e528644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5ef0f-f1be-4a4f-b0d6-6ead97c1c04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "428fddbf-e66c-4fe5-afb4-38d77e68396e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter clean_data based on NDVI and EVI values\n",
    "clean_data = clean_data[(clean_data['NDVI'] >= 0.2) & (clean_data['EVI'] >= -1) & (clean_data['EVI'] <= 1)].reset_index(drop=True)\n",
    "\n",
    "# Replace crop labels with numeric values\n",
    "clean_data['Crop'] = clean_data['Crop'].replace({'Soyabean': '5', 'Corn': '1'})\n",
    "\n",
    "# Convert specific columns to category dtype\n",
    "clean_data[['OBJECTID', 'Crop', 'month']] = clean_data[['OBJECTID', 'Crop', 'month']].astype('category')\n",
    "\n",
    "# Filter non-soybean data for a specific date\n",
    "non_soya = clean_data[(clean_data['Crop'] == '5') & (clean_data['Date'] == '2019-06-01 00:00:00')].reset_index(drop=True)\n",
    "\n",
    "# Create a boolean mask for rows not in non_soya\n",
    "mask = ~clean_data.set_index(non_soya.columns.tolist()).index.isin(non_soya.set_index(non_soya.columns.tolist()).index)\n",
    "\n",
    "# Filter clean_data using the mask\n",
    "ARD = clean_data[mask].reset_index(drop=True)\n",
    "\n",
    "# Select training and validation subsets\n",
    "train = ARD[(ARD['Region'] == 'Piatt') & (ARD['Date'].dt.year < 2023)].reset_index(drop=True)\n",
    "val = ARD[ARD['Date'].dt.year == 2023].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create a dictionary to store all the test subsets\n",
    "test_dict = {\n",
    "    'test_1': ARD[ARD['Region'] == 'Crawford'].reset_index(drop=True),\n",
    "    'test_2': ARD[ARD['Region'] == 'Occitanie'].reset_index(drop=True),\n",
    "    'test_3': ARD[ARD['Region'] == 'Campo Verde'].reset_index(drop=True),\n",
    "}\n",
    "\n",
    "# Find the date when each farm had the highest NDVI value for test_2\n",
    "max_NDVI_dates = test_dict['test_2'].loc[\n",
    "    test_dict['test_2'].groupby('OBJECTID', observed=True)['NDVI'].idxmax()\n",
    "][['OBJECTID', 'Date']]\n",
    "\n",
    "# Merge the maximum NDVI dates back into the original DataFrame\n",
    "merged_df = test_dict['test_2'].merge(\n",
    "    max_NDVI_dates,\n",
    "    on='OBJECTID',\n",
    "    suffixes=('', '_Max'),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Helper function to create DataFrames based on the peak NDVI date\n",
    "def create_dataframes(df, crop_code):\n",
    "    maturity_df = df[(df['Date'] > df['Date_Max']) & (df['Crop'] == crop_code)].drop(columns='Date_Max').reset_index(drop=True)\n",
    "    before_peak_df = df[(df['Date'] < df['Date_Max']) & (df['Crop'] == crop_code)].drop(columns='Date_Max').reset_index(drop=True)\n",
    "    peak_df = df[(df['Date'] == df['Date_Max']) & (df['Crop'] == crop_code)].drop(columns='Date_Max').reset_index(drop=True)\n",
    "    early_df = pd.concat([before_peak_df, peak_df]).reset_index(drop=True)\n",
    "    return maturity_df, early_df\n",
    "\n",
    "# Create DataFrames for Corn and Soybean based on the peak NDVI date\n",
    "maturity_corn, early_corn = create_dataframes(merged_df, '1')\n",
    "maturity_soya, early_soya = create_dataframes(merged_df, '5')\n",
    "\n",
    "# List of NDVI ranges\n",
    "ndvi_ranges = [round(i / 10, 1) for i in range(2, 9)]  # NDVI ranges from 0.2 to 0.8\n",
    "ndvi_ranges.append(0.8)  # Last range is >= 0.8\n",
    "\n",
    "# Helper function to add DataFrames to test_dict based on NDVI ranges\n",
    "def add_ndvi_dataframes(df, crop_name, phase_name):\n",
    "    for i, ndvi in enumerate(ndvi_ranges):\n",
    "        if i < len(ndvi_ranges) - 1:  # For all ranges except the last one\n",
    "            ndvi_df = df[(df['NDVI'] >= ndvi) & (df['NDVI'] < ndvi + 0.1)].reset_index(drop=True)\n",
    "        else:  # For the last range, use >= 0.8\n",
    "            ndvi_df = df[df['NDVI'] >= ndvi].reset_index(drop=True)\n",
    "        test_dict[f\"{crop_name}_{ndvi}_{phase_name}\"] = ndvi_df\n",
    "\n",
    "# Add 'early' and 'maturity' DataFrames for Corn and Soybean to test_dict\n",
    "add_ndvi_dataframes(early_corn, 'corn', 'early')\n",
    "add_ndvi_dataframes(early_soya, 'soya', 'early')\n",
    "add_ndvi_dataframes(maturity_corn, 'corn', 'maturity')\n",
    "add_ndvi_dataframes(maturity_soya, 'soya', 'maturity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab4f90-acc2-4f24-a254-d17ed440ad67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85d644-824e-4a29-9961-27cacaff1fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "features = ['Crop', 'VV', 'VH', 'NRPB', 'RVI', 'Ratio', 'Span', 'cum_GDD']\n",
    "VIs = ['NDVI', 'EVI']\n",
    "\n",
    "# Select features and labels from the train subset for feature selection\n",
    "X, Y = train[features], train[VIs]\n",
    "\n",
    "# Plot a correlation heatmap of independent variables\n",
    "sns.heatmap(X.corr(), cmap='RdYlGn', linewidths=0.30, annot=True, fmt='.2f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16848122-1495-4d86-a668-bef79f71609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a figure for the plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for idx, VI in enumerate(VIs):\n",
    "    # Select features and the current label from the train subset\n",
    "    X = train[features]\n",
    "    Y = train[VI]\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMRegressor(random_state=42, importance_type='gain', verbose=-1, force_col_wise=True)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # Get feature importances from the model\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # Create subplots for feature importance\n",
    "    plt.subplot(1, len(VIs), idx + 1)\n",
    "    plt.bar(range(len(feature_importances)), feature_importances[indices], color='skyblue')\n",
    "    plt.xticks(range(len(feature_importances)), X.columns[indices], rotation=90)\n",
    "    plt.title(f'Feature Importance for {VI}')\n",
    "    plt.xlim(-1, len(feature_importances))\n",
    "    plt.ylim(0, 10000)  # Adjust as needed\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a847da-b4e2-46a9-b0ba-1829de2343ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style('whitegrid')\n",
    "X_test = {}  # For storing independent variables of the test sets\n",
    "\n",
    "# Create subplots for side-by-side plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(VIs), figsize=(12, 5))\n",
    "\n",
    "for i, VI in enumerate(VIs):\n",
    "    # Select features and the current label from the train subset\n",
    "    X, Y = train[features], train[VI]\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMRegressor(random_state=42, importance_type='gain', verbose=-1, force_col_wise=True)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(model, X, Y, random_state=42, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    # Create a DataFrame for easier analysis\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': perm_importance.importances_mean * 10\n",
    "    })\n",
    "\n",
    "    # Select features with importance >= 0.10\n",
    "    sel_features = importance_df[importance_df['Importance'] >= 0.10]['Feature'].tolist()\n",
    "\n",
    "    # Filter features for training and validation sets\n",
    "    x_train, x_val = train[sel_features], val[sel_features]\n",
    "\n",
    "    # Filter features for testing\n",
    "    for test, df in test_dict.items():\n",
    "        X_test[test] = df[sel_features]\n",
    "\n",
    "    # Plot permutation importance\n",
    "    sorted_idx = np.argsort(importance_df['Importance'])\n",
    "    axes[i].barh(importance_df['Feature'][sorted_idx], importance_df['Importance'][sorted_idx], color='skyblue')\n",
    "    axes[i].set_xlabel(\"Permutation Importance\")\n",
    "    axes[i].set_title(f\"Permutation Importance for {VI}\")\n",
    "\n",
    "    # Optional: Print the selected features for each label\n",
    "    print(f\"Selected features for {VI}: {sel_features}\")\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d00e5e-bc0b-4344-8462-b714eece0d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a76aae-e1f6-4111-aa6e-b15e29f8fab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550fe80f-137f-48c9-9fbe-dac7a1dd3c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select labels for train, validation, and test sets\n",
    "y_train, y_val = train[VIs], val[VIs]\n",
    "Y_test = {test: df[VIs] for test, df in test_dict.items()}\n",
    "\n",
    "# To store the best hyperparameters and MAE for each VI\n",
    "best_hps = []\n",
    "best_MAE = []\n",
    "\n",
    "# Suppress logging\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial, VI):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.05, 0.3, step=0.01),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100, step=10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1, step=0.1),\n",
    "    }\n",
    "\n",
    "    # Initialize and fit the model on the training set\n",
    "    lgb_model = lgb.LGBMRegressor(**params, random_state=42)\n",
    "    lgb_model.fit(x_train, y_train[VI])\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred_val = lgb_model.predict(x_val)\n",
    "\n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(y_val[VI], y_pred_val)\n",
    "    \n",
    "    return MAE\n",
    "\n",
    "# Optimize hyperparameters for each VI\n",
    "for VI in VIs:\n",
    "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.RandomSampler(seed=42))\n",
    "    study.optimize(lambda trial: objective(trial, VI), n_trials=100)\n",
    "\n",
    "    best_hps.append(study.best_params)\n",
    "    best_MAE.append(study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1815e7-9181-4f32-a090-0824c56ccf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c2c5-44f8-4d44-8fa3-0c2703dc8ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store predictions, models, and residuals\n",
    "VI_preds, models, residuals = [], [], []\n",
    "results = []\n",
    "\n",
    "# Function to calculate root mean squared error\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Flag to indicate if columns should be dropped\n",
    "drop_columns = False\n",
    "\n",
    "# Loop through each test set and corresponding labels\n",
    "for i, (test_name, (x, y)) in enumerate(zip(X_test.keys(), zip(X_test.values(), Y_test.values()))):\n",
    "    # Check if the iteration is after the third key-value pair\n",
    "    if i >= 3:\n",
    "        drop_columns = True\n",
    "    \n",
    "    for VI, best_params in zip(VIs, best_hps):\n",
    "        # Adjust features if the drop_columns flag is set\n",
    "        if drop_columns:\n",
    "            x_adj = x.drop(x.columns[0], axis=1)\n",
    "            x_train_adj = x_train.drop(x_train.columns[0], axis=1)\n",
    "        else:\n",
    "            x_adj = x\n",
    "            x_train_adj = x_train\n",
    "\n",
    "        # Fit the model\n",
    "        model = lgb.LGBMRegressor(random_state=42, importance_type='gain', **best_params)\n",
    "        model.fit(x_train_adj, y_train[VI])\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(x_adj)\n",
    "        y_pred_df = pd.DataFrame(y_pred, columns=[f'{VI}_pred'])\n",
    "        \n",
    "        # Calculate residuals\n",
    "        residual = y[VI] - y_pred\n",
    "        residual_df = pd.DataFrame(residual, columns=[f'{VI}_res'])\n",
    "        \n",
    "        # Store predictions and residuals\n",
    "        VI_preds.append(y_pred_df)\n",
    "        residuals.append(residual_df)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mae = mean_absolute_error(y[VI], y_pred)\n",
    "        rmse = root_mean_squared_error(y[VI], y_pred)\n",
    "        r2 = model.score(x_adj, y[VI])\n",
    "        bias = np.mean(residual)\n",
    "\n",
    "        # Store the result for this label and test set\n",
    "        results.append({\n",
    "            'test_name': test_name,\n",
    "            'VI': VI,\n",
    "            'MAE': round(mae, 3),\n",
    "            'RMSE': round(rmse, 3),\n",
    "            'bias': round(bias, 4),\n",
    "            'R2': round(r2, 3),\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['test_name'] = results_df['test_name'].replace({'test_1': 'Crawford', 'test_2': 'Occitanie', 'test_3': 'Campo Verde'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17e3f9-44e8-482d-8420-67ac62429cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cab6e-6569-4d86-9340-ee03c1051e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_eva= results_df.head(6) # evaluation metrics for geographical area analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f240266-5bf5-4a01-b64c-0495f6dbc0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "805861d5-3134-43cc-bc98-64f1979a85f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract test results containing '0.' and derive range and stage columns\n",
    "stages_results = results_df[results_df['test_name'].str.contains('0.')]\n",
    "stages_results['range'] = stages_results['test_name'].str[5:8].replace({'0.8': '>=0.8'})\n",
    "stages_results['stage'] = stages_results['test_name'].str[9:]\n",
    "\n",
    "# Filter for corn and soya tests with NDVI\n",
    "corn_NDVI = stages_results[(stages_results['test_name'].str.contains('corn')) & (stages_results['VI'] == 'NDVI')]\n",
    "soya_NDVI = stages_results[(stages_results['test_name'].str.contains('soya')) & (stages_results['VI'] == 'NDVI')]\n",
    "\n",
    "# Define color palette\n",
    "colors = ['#2E8B57', '#ff7f0e']  # green and orange\n",
    "\n",
    "# Group the dataframe by stage \n",
    "grouped_df = corn_NDVI.groupby('stage')\n",
    "\n",
    "# Create subplots for two bar graphs side by side with shared y-axis\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "# Loop through each unique stage value and plot\n",
    "for idx, (stage, data) in enumerate(grouped_df):\n",
    "    axs[idx].bar(data['range'], data['MAE'], color=colors[idx])\n",
    "    if idx == 1:\n",
    "        axs[idx].invert_xaxis()  # Reverse the x-axis for the second plot\n",
    "        axs[idx].tick_params(left=False, labelleft=False)  # Remove y-axis ticks for the second plot\n",
    "    axs[idx].grid(True, axis='y', linestyle='--', linewidth=0.5)  # Add horizontal grid lines only\n",
    "\n",
    "# Set common labels and y-axis range\n",
    "fig.text(0.53, 0.005, 'Range', ha='center')\n",
    "fig.text(0.001, 0.5, 'MAE', va='center', rotation='vertical')\n",
    "for ax in axs:\n",
    "    ax.set_ylim(0.00, 0.20)\n",
    "\n",
    "# Adjust layout to prevent overlap and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11809817-0cbc-4e7c-84ee-897b5f287fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebadaddc-bec9-42ea-82c1-2cb71e3f040b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "11e7a2de-c5cf-4935-8ec5-a74ba1aa5c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter test_2 for OBJECTIDs that appear 10 times\n",
    "ten_dates = test_dict['test_2'][test_dict['test_2']['OBJECTID'].map(test_dict['test_2']['OBJECTID'].value_counts()) == 10]\n",
    "\n",
    "# Filter max_NDVI_dates for specific date and match OBJECTIDs in both datasets\n",
    "aug_17 = max_NDVI_dates[max_NDVI_dates.Date == '2019-08-17 00:00:00']\n",
    "aug_ten = ten_dates[ten_dates.OBJECTID.isin(aug_17.OBJECTID)].reset_index(drop=True)\n",
    "\n",
    "# Concatenate predictions and residuals along columns\n",
    "pred_res = pd.concat([pd.concat(VI_preds, axis=1), pd.concat(residuals, axis=1)], axis=1)\n",
    "\n",
    "# Combine data for visualization\n",
    "vis_data = pd.concat([test_dict['test_2'][['OBJECTID', 'Date']], pred_res], axis=1)\n",
    "aug_ten = aug_ten.merge(vis_data, on=['OBJECTID', 'Date'])\n",
    "\n",
    "# Drop unwanted columns\n",
    "unwanted = ['VV', 'VH', 'NRPB', 'RVI', 'Ratio', 'Span']\n",
    "aug_ten.drop(unwanted, axis=1, inplace=True)\n",
    "\n",
    "# Select and group corn data\n",
    "corn_sel = aug_ten[aug_ten['Crop'] == '1'].sample(n=4, random_state=42)\n",
    "corn_sel_all = aug_ten[aug_ten.OBJECTID.isin(corn_sel.OBJECTID)].reset_index(drop=True)\n",
    "corn_group = corn_sel_all.groupby('OBJECTID', observed=True)\n",
    "\n",
    "# Select and group soya data\n",
    "soya_sel = aug_ten[aug_ten['Crop'] == '5'].sample(n=4, random_state=10)\n",
    "soya_sel_all = aug_ten[aug_ten.OBJECTID.isin(soya_sel.OBJECTID)].reset_index(drop=True)\n",
    "soya_group = soya_sel_all.groupby('OBJECTID', observed=True)\n",
    "\n",
    "# Create a figure and axes with 2 rows and 2 columns (4 subplots)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the grouped data for each OBJECTID\n",
    "for idx, (objectid, group) in enumerate(soya_group):\n",
    "    # Plot the time series for the actual NDVI and predicted NDVI for each OBJECTID\n",
    "    axes[idx].plot(group['Date'], group['NDVI'], label='Actual NDVI', color='blue', marker='o')\n",
    "    axes[idx].plot(group['Date'], group['NDVI_pred'], label='Predicted NDVI', color='red', marker='x')\n",
    "    # Rotate date labels for readability\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "fig.legend(['Actual NDVI', 'Predicted NDVI'], loc='upper center', ncol=2)\n",
    "\n",
    "# Add shared axis labels for the entire figure\n",
    "fig.supxlabel('Date', fontsize=14)\n",
    "fig.supylabel('NDVI', fontsize=14)\n",
    "\n",
    "# Adjust layout to avoid overlapping\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space at the top for the legend\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad4c8c-31af-470c-af42-f4cb3430a625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57230587-d3bc-46fd-b614-fd8b10b7e248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d8a037ad-718b-42b9-9a51-d0ae1947512a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate predictions and residuals along columns\n",
    "pred_res = pd.concat([pd.concat(VI_preds, axis=1), pd.concat(residuals, axis=1)], axis=1)\n",
    "\n",
    "# Combine data for visualization\n",
    "vis_data = pd.concat([test_dict['test_1'][['OBJECTID', 'Date']], pred_res], axis=1)\n",
    "g_data_1 = g_data.merge(vis_data, on=['OBJECTID', 'Date'])\n",
    "\n",
    "# Group the data by date\n",
    "vis_groups = g_data_1.groupby('Date')\n",
    "\n",
    "# Define colormap\n",
    "cmap = 'tab20_r'\n",
    "\n",
    "def plot_ndvi_groups(vis_groups, date: str, cmap: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots NDVI values for specified date across multiple columns.\n",
    "\n",
    "    Args:\n",
    "    vis_groups (DataFrameGroupBy): Grouped data containing NDVI values.\n",
    "    date (str): The date for which to plot the NDVI values.\n",
    "    cmap (str): Colormap to use for the plots.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays the plots.\n",
    "    \"\"\"\n",
    "    NDVI_columns = vis_groups.get_group(date).filter(like='NDVI')\n",
    "    fig, axs = plt.subplots(1, len(NDVI_columns.columns), figsize=(15, 6))\n",
    "\n",
    "    for i, column in enumerate(NDVI_columns.columns):\n",
    "        vis_groups.get_group(date).plot(\n",
    "            y=column,\n",
    "            ax=axs[i],\n",
    "            legend=False,\n",
    "            cmap=cmap,\n",
    "            vmax=1.0,\n",
    "            vmin=0.1\n",
    "        ).set_title(f'{column}, {date[:10]}')\n",
    "    \n",
    "    # Add colorbar to the figure\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0.0, vmax=1.0))\n",
    "    fig.colorbar(sm, ax=axs, fraction=0.04, pad=0.1, location='bottom').set_label('NDVI')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot NDVI groups for each date\n",
    "for name, group in vis_groups:\n",
    "    plot_ndvi_groups(vis_groups, name, cmap)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
